{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction_to_python_pytorch_plotting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rWfIVZM2lfN3",
        "o4ldSPEqDBc-",
        "_ITMycMNDHak",
        "u1_6N2beDN-6",
        "NDEnWVZ4DgGu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KBLin1996/Advertise_Click_Prediction/blob/master/introduction_to_python_pytorch_plotting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcC_8S5ukVBj"
      },
      "source": [
        "## **Python, Pytorch and Plotting**\n",
        "\n",
        "In our class we will be using Google Colab / Jupyter notebooks and python for the assignments so it is important to be confident with both ahead of time. We will additionally be using a matrix (tensor) manipulation library similar to numpy called pytorch. We will learn during the class why pytorch is much more than a matrix maninpulation package but for the purposes of this tutorial we will just use it as a convenient tool for array/matrix/tensor manipulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvsdeMU8kYSL"
      },
      "source": [
        "import torch  # This imports the torch library.\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt  # This is python's popular plotting library."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWfIVZM2lfN3"
      },
      "source": [
        "### **1. Creating, Indexing, and Displaying 2D Tensors (Matrices)**\n",
        "The main data structure you have to get yourself familiar during this course is the pytorch Tensor, or put simply, a multidimensional array. We will create here a few tensors, manipulate them and display them. The indexing operations inside a tensor in pytorch is similar to indexing in numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlT6TZb8krYG",
        "outputId": "7c1dc87b-0767-4f1b-f37c-d4ce982cd76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "myTensor = torch.zeros(7, 7, dtype=torch.float)\n",
        "myTensor[:, :] = 0   # Assign zeros everywhere in the matrix.\n",
        "myTensor[3, 3] = 1   # Assign one in position 3, 3\n",
        "myTensor[:2, :] = 1   # Assign ones on the top 2 rows.\n",
        "myTensor[-2:, :] = 1    # Assign ones on the bottom 2 rows.\n",
        "\n",
        "# Show the tensor.\n",
        "def showTensor(aTensor):\n",
        "    matplotlib.rc('image', cmap = 'viridis') # Colorful colormap.\n",
        "    plt.figure()\n",
        "    plt.imshow(aTensor)\n",
        "    plt.colorbar()\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "    \n",
        "showTensor(myTensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAD8CAYAAAA11GIZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUzklEQVR4nO3dfbBdV13G8e/TtJC+Ju0EriUJtjMExk60hWaiWKdUsW0KTOuMTG0ZUBC8Mw7FYBu0qICWcbRmRKt20GupvBNrsU5G00ZmKEYcW5JAKUlfIAY0NyChL7bEtrS59/GPs4OHO7n37Hve7jp3P5+ZPT1nn7X3byWd/rrW2muvJdtERJTmuIWuQETEsSQ5RUSRkpwiokhJThFRpCSniChSklNEFCnJKSJ6JulWSYck7Znld0n6M0n7JN0v6RWd7pnkFBH98GFgwxy/XwasqY5x4IOdbpjkFBE9s70DeGyOIlcAH3XLPcBySWfOdc/j+1nBo1acscRnrT6hq2sPP7OCU5Y+0ucaJXZiL67Y3zjwHI88NqVe6nDpT5/sRx+bqlV29/3f2ws803ZqwvbEPMKtBA60fZ+szn1rtgsGkpzOWn0CX9i+uqtrd+zZyIVrb+pzjRI7sRdX7PWXHuhcqINHH5viC9tfXKvskjO/9oztdT0HnYeBJKeIKJ+BaaaHFe4g0N5iWVWdm1XGnCIaypjnPFXr6IOtwC9WT+1+AnjC9qxdOkjLKaLR+tVykvQp4CJghaRJ4H3ACQC2/xLYBrwG2Ac8Bbyl0z2TnCIaypipPi2ZZPvqDr8bePt87pnkFNFg05S7nluSU0RDGZhKcoqIEqXlFBHFMfBcwct0JzlFNJRxunURUSDDVLm5KckpoqlaM8TLVWuGuKQNkh6u1mK5ftCViohhEFM1j4XQseUkaQlwM3AxrTeJd0raavuBQVcuIganNSC+MImnjjotp/XAPtv7bT8LbKG1NktEjLDWPKcRbjlx7HVYfnxmIUnjtFa4Y2xsOTv2bOyqQoefHuv62l4ldmKPTuxNfanHdMEtp74NiFcLT00ArDt3qbtdq2aU19hJ7MQehdhHHW05lapOcpr3OiwRUT4jpgpeNalOctoJrJF0Nq2kdBXwhoHWKiKGYqS7dbaPSLoG2A4sAW61vXfgNYuIgTLiWS9Z6GrMqtaYk+1ttBaLiohFojUJc7S7dRGxSI36gHhELEK2mHJaThFRoOm0nCKiNK0B8XJTQLk1i4iByoB4RBRrapTnOUXE4rQYZohHxCI1nad1EVGa1ou/SU4RURgjnhv111ciYvGxySTMiCiRMgkzIspj0nKKiEJlQDwiimM02ovNRcTi1NoaqtwUUG7NImLAFm7bpzqSnCIaymSGeEQUquSWU7lpMyIGyhbTPq7W0YmkDZIelrRP0vXH+P3Fku6W9CVJ90t6Tad7puUU0VCtAfHeX1+RtAS4GbiY1o7gOyVttf1AW7HfAW6z/UFJ59DaMOWsue6b5BTRWH1bQ3w9sM/2fgBJW4ArgPbkZOC06vMy4JudbtqxZpJulXRI0p55VzkiitUaEFetA1ghaVfbMd52q5XAgbbvk9W5dr8LvFHSJK1W0zs61a9Oy+nDwF8AH61RNiJGyDxmiD9ie10Poa4GPmz7jyW9EviYpLW2p2e7oM6OvzskndVDpSKiQH2cIX4QWN32fVV1rt1bgQ0Atv9d0lJgBXBotpv2bcypauaNA4yNLWfHno1d3efw02NdX9urxE7s0Ym9qS/16NMGBzuBNZLOppWUrgLeMKPMfwGvBj4s6UeApcB35rpp35KT7QlgAmDduUt94dqburrPjj0b6fbaXiV2Yjch9lE2PDfde3KyfUTSNcB2YAlwq+29km4AdtneClwH/LWkX6c13PVm257rvnlaF9FQrW5df6Y62t5Ga6C7/dx72z4/AFwwn3smOUU02EjPEJf0KeDfgZdJmpT01sFXKyIGbZ5TCYauztO6q4dRkYgYtv516wYh3bqIBssa4hFRnNbTumwNFRGFyTK9EVGsdOsiojhHn9aVKskposHytC4iimOLI0lOEVGidOsiojiNHHP66v0ncemLzuvq2is3n8TvX9Ldtb1K7MQeldhf9aN9qUfjklNElC/znCKiWJnnFBHFseFIHxabG5Qkp4gGS7cuIoqTMaeIKJaTnCKiRBkQj4ji2Blziogiiak8rYuIEmXMKSKK08h36yJiBLg17lSqOvvWrZZ0t6QHJO2VtDCby0dE302jWsdCqNNyOgJcZ/uLkk4Fdkv6TLW9cESMKI/6gLjtbwHfqj5/V9KDwEogySlixJXcrZPnUTtJZwE7gLW2n5zx2zgwDnD6sjPOv/E9m7uq0OmrlvH45BNdXdurxE7sUYl93aZNPOnHeupvnbTmRX7JB95Wq+xXLn//btvreok3X7UHxCWdAnwaeOfMxARgewKYADhNZ/i2d93ZVYWu3HwZ3V7bq8RO7CbEPspeBFMJJJ1AKzF9wvbfD7ZKETEsIz2VQJKADwEP2v7A4KsUEcNS8phTnZbTBcCbgK9Iuq8691u2tw2uWhExaEZMj/jTus9Dwa8uR0TXCm44dZ6EGRGLVDUgXufoRNIGSQ9L2ifp+lnKXNk2mfuTne6Z11cimqwPTSdJS4CbgYuBSWCnpK3tE7UlrQHeDVxg+3FJL+x037ScIhqsTy2n9cA+2/ttPwtsAa6YUeZXgJttP96K60OdbprkFNFQBqanVesAVkja1XaMt91qJXCg7ftkda7dS4GXSvo3SfdI2tCpfunWRTSVgfrznB7pcYb48cAa4CJgFbBD0o/a/p/ZLkjLKaLB7HpHBweB1W3fV1Xn2k0CW20/Z/vrwFdpJatZJTlFNJlrHnPbCayRdLak5wFXAVtnlPkHWq0mJK2g1c3bP9dN062LaKx60wQ6sX1E0jXAdmAJcKvtvZJuAHbZ3lr9domkB4Ap4F22H53rvklOEU3Wp1mY1Rsj22ace2/bZwPXVkctSU6LxPZv3te50Cx27HlVT9cDXPqi83q6PhaAwdPlvvyR5BTRaElOEVGigl+uS3KKaLIkp4gozvwmYQ5dklNEg436YnMRsVjlaV1ElEhpOUVEceq9mrJgkpwiGksZEI+IQqXlFBFFml7oCswuySmiqUZ9npOkpcAO4PlV+dttv2/QFYuIwRv1p3XfA37G9uFqW/LPS7rT9j0DrltEDNooJ6dqHZbD1dcTqqPgP1JELAZyjfnr1b5Uu4GX0Nre5TePUWYcGAc4fdkZ59/4ns1dVej0Vct4fPKJrq7t1SjHXnPuU11fe/jpMU458dtdXw/wtS+f1NV1o/x3vpCxr9u0iSf9WE8DRs9/8Wqv3PTOWmW/vnHT7h43OJi3WgPitqeA8yQtB+6QtNb2nhllJoAJgNN0hm97151dVejKzZfR7bW9GuXYvS02t5EL197U9fUAv39Jd4vNjfLf+ajG/j5T9Osr89rgoNrG5W6g455TETEC+rPBwUB0TE6SXlC1mJB0Iq0thx8adMUiYvDkesdCqNOtOxP4SDXudBxwm+1/HGy1ImIoCn60Vedp3f3Ay4dQl4gYtlFOThGxOC1kl62OJKeIJiv4aV2SU0SDpeUUEWVKcoqI4mTMKSKKleQUESVSwYvNzev1lYiIYUnLKaLJ0q2LiOJkQDyG4dIXdbdkCcCVm0/qesmTGHFJThFRpCSniCiNyNO6iChRzbWc6oxLSdog6WFJ+yRdP0e5n5dkSR2X/E1yimiyPqyEWa31djNwGXAOcLWkc45R7lRgI3BvnaolOUU0WX+W6V0P7LO93/azwBbgimOUez9wI/BMnaolOUU02Dy6dSsk7Wo7xttusxI40PZ9sjr3/3GkVwCrbf9T3bplQDyiyeo/rXuk262hJB0HfAB483yuS3KKaCr37WndQWB12/dV1bmjTgXWAp+TBPBDwFZJl9veNdtNk5wimqw/85x2AmsknU0rKV0FvOH7IewngBVHv0v6HLBprsQEGXOKaLR+TCWwfQS4BtgOPEhrh6a9km6QdHm3dUvLKaLJ+jRD3PY2YNuMc++dpexFde5Zu+UkaYmkL0nKnnURi0HdaQQFb6p51EZaTbbTBlSXiBgiUfaqBLVaTpJWAa8FbhlsdSJimErejlx258iSbgf+gNYjwU22X3eMMuPAOMDpy844/8b3bO6qQqevWsbjk090dW2vEjuxRyX2dZs28aQf62nTuZPGVnvNVdfWKnv/n127u9t5Tt3q2K2T9DrgkO3dki6arZztCWAC4DSd4dvedWdXFbpy82V0e22vEjuxmxD7BxTcrasz5nQBcLmk1wBLgdMkfdz2GwdbtYgYqMJXwuw45mT73bZX2T6L1uSqzyYxRSwSi+RpXUQsMiUvNjev5GT7c8DnBlKTiBi6krt1aTlFNNUCdtnqSHKKaLIkp4goTekzxJOcIhpM0+VmpySniKbKmFNElCrduogoU5JTRJQoLaeIKFOSU0QUp3+7rwzEQJLTS3/sKbZvv6+ra3fseRXbv9ndtb1K7MQeldjrL32q5zpknlNElKvGYpMLJckposHScoqI8mQSZkSUqnED4hExGpKcIqI8JgPiEVGmDIhHRJmSnCKiNJmEGRFlsrPYXEQUqtzcVC85SfoG8F1gCjgy7D3TI2IwFku37qdtPzKwmkTEcBlIty4iilRubkKuMQlL0teBx2n9Uf7K9sQxyowD4wBjY8vP3/Lx93dVocNPj3HKid/u6tpeJXZij0rsTddtYteXn1EvdTh12Sqf/5O/Vqvsv9z1m7vnGs6RtAG4CVgC3GL7D2f8fi3wNuAI8B3gl23/51wx67acfsr2QUkvBD4j6SHbO9oLVAlrAmDduUt94dqbat76B+3Ys5Fur+1VYid2E2K368fTOklLgJuBi4FJYKekrbYfaCv2JWCd7ack/SrwR8AvzHXf4+oEt32w+uch4A5g/fz/CBFRFM/jmNt6YJ/t/bafBbYAV/xAKPtu20dXyLsHWNXpph2Tk6STJZ169DNwCbCnY3UjomitSZiudQArJO1qO8bbbrUSOND2fbI6N5u3And2ql+dbt0YcIeko+U/afuuGtdFROnqr0rwSD+mEEl6I7AOeFWnsh2Tk+39wLm9VioiyqP+rEpwEFjd9n1Vde4HY0k/C/w28Crb3+t001pjThGxCPVvzGknsEbS2ZKeB1wFbG0vIOnlwF8Bl1dj1x1lnlNEY/Xn3TrbRyRdA2ynNZXgVtt7Jd0A7LK9FdgMnAL8XTVE9F+2L5/rvklOEU3Wp8XmbG8Dts049962zz8733smOUU0VRM31YyIEZFleiOiSOXmpiSniCbTdLn9uiSniKYy85mEOXRJThENJdyvSZgDkeQU0WRJThFRpCSniChOxpwiolR5WhcRBXK6dRFRIJPkFBGFKrdXl+QU0WSZ5xQRZUpyioji2DBVbr8uySmiydJyiogiJTlFRHEM9GEN8UGptfuKpOWSbpf0kKQHJb1y0BWLiEEzeLresQDqtpxuAu6y/fpq65eTBliniBgGM9oD4pKWARcCbwao9kJ/drDVioihKHjMSe5QOUnnARPAA7R2/t0NbLT9vzPKjQPjAGNjy8/f8vH3d1Whw0+PccqJ3+7q2l4ldmKPSuxN121i15efUS91WPa8Mf/kC3+hVtm7Dv757n5sRz4fdbp1xwOvAN5h+15JNwHXA+9pL2R7glYSY925S33h2pu6qtCOPRvp9tpeJXZiNyH2/yv7xd86A+KTwKTte6vvt9NKVhExygxMT9c7FkDH5GT7v4EDkl5WnXo1rS5eRIw6u96xAOo+rXsH8InqSd1+4C2Dq1JEDMcieH3F9n3AUAfDImLADF6gOUx1ZIZ4RJMVPEM8ySmiyQp+WpfkFNFU9oI9iasjySmiydJyiojyGE9NLXQlZpXkFNFUhS+ZkuQU0WQFTyWotZ5TRCw+BjztWkcnkjZIeljSPknXH+P350v62+r3eyWd1emeSU4RTeX+LDYnaQlwM3AZcA5wtaRzZhR7K/C47ZcAfwLc2Kl6SU4RDeapqVpHB+uBfbb3V+u9bQGumFHmCuAj1efbgVdLmnPJl47rOXVD0neA/+zy8hXAI32sTmIn9mKM/cO2X9BLBSTdVdWjjqXAM23fJ6plkpD0emCD7bdV398E/Ljta9pi7anKTFbf/6MqM+vfwUAGxHv5S5O0a9iLWiV2Yjcp9lG2Nyxk/E7SrYuIXh0EVrd9X1WdO2YZSccDy4BH57ppklNE9GonsEbS2dWySlcBW2eU2Qr8UvX59cBn3WFMqcR5ThOJndiJPTpsH5F0DbAdWALcanuvpBuAXba3Ah8CPiZpH/AYrQQ2p4EMiEdE9CrduogoUpJTRBSpqOTUaQr8AOPeKulQNRdjqCStlnS3pAck7ZW0cYixl0r6gqQvV7F/b1ix2+qwRNKXJP3jkON+Q9JXJN0nadeQYy+XdLukhyQ9KOmVw4w/KooZc6qmwH8VuJjWdlQ7gattD3ynF0kXAoeBj9peO+h4M2KfCZxp+4uSTqW1aenPDenPLeBk24clnQB8ntaGqfcMOnZbHa6ltT79abZfN8S43wDWzTUJcICxPwL8q+1bqqdbJ9n+n2HXo3QltZzqTIEfCNs7aD1BGDrb37L9xerzd4EHgZVDim3bh6uvJ1TH0P5vJWkV8FrglmHFXGiSlgEX0np6he1nk5iOraTktBI40PZ9kiH9R1qK6k3tlwP3zl2yrzGXSLoPOAR8pm3z1GH4U+A3gIVYt8PAP0vaLWl8iHHPBr4D/E3Vnb1F0slDjD8ySkpOjSbpFODTwDttPzmsuLanbJ9Ha1bveklD6dZKeh1wyPbuYcQ7hp+y/Qpab9K/veraD8PxtHbM/qDtlwP/CwxtfHWUlJSc6kyBX5Sq8Z5PA5+w/fcLUYeqa3E3MKz3rS4ALq/GfrYAPyPp40OKje2D1T8PAXfQGlYYhklgsq2FejutZBUzlJSc6kyBX3SqQekPAQ/a/sCQY79A0vLq84m0HkY8NIzYtt9te5Xts2j9u/6s7TcOI7akk6uHD1RdqkuAoTyptf3fwAFJL6tOvRoY+MOPUVTM6yuzTYEfRmxJnwIuAlZImgTeZ/tDw4hNqwXxJuAr1dgPwG/Z3jaE2GcCH6melB4H3GZ7qI/0F8gYcEe1nNDxwCdt3zXE+O8APlH9T3g/8JYhxh4ZxUwliIhoV1K3LiLi+5KcIqJISU4RUaQkp4goUpJTRBQpySkiipTkFBFF+j/UBAyBMNTXpwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YouRIlcXnOAH"
      },
      "source": [
        "If you are familiar with numpy indexing then everything above should look familiar. We can access and modify the tensor using indexing of single elements myTensor[index1, index2], or ranges of indices myTensor[index1_1 : index1_2, index2_1 : index2_2]. Notice that the indices could contain positive or negative values, where negative values index counting from end to beginning. Also notice that we can convert a pytorch tensor to a numpy array easily using the .numpy() method.\n",
        "\n",
        "**Exercise: ** Use indexing operations to generate the following patterns:\n",
        "\n",
        "![alt text](http://www.cs.virginia.edu/~vicente/recognition/notebooks/plt3.png)\n",
        "![alt text](http://www.cs.virginia.edu/~vicente/recognition/notebooks/plt1.png)\n",
        "![alt text](http://www.cs.virginia.edu/~vicente/recognition/notebooks/plt2.png)\n",
        "\n",
        "I also strongly recommend you to look over the definitions of the index(), index_add_(), index_fill(), index_fill_(), index_copy_(), index_select(), scatter_(), and gather() operations, as you will see them in other people's code, and maybe you will find them useful for your own projects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Reshaping, Slicing, and Cloning Tensors.**\n",
        "We will also be often needing to extract subsections of a tensor or reshaping the tensor in different ways. Here are examples for a couple of useful operations."
      ],
      "metadata": {
        "id": "o4ldSPEqDBc-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xxn_iDBl2WY",
        "outputId": "96eaa14e-f528-4e9e-e8d4-86ff8047ae1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This creates one-dimensional tensor (array)\n",
        "myTensor = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
        "print(myTensor)\n",
        "print(myTensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3., 4.])\n",
            "torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJHHQLAoon6t",
        "outputId": "eadc7141-f62e-4cc4-e180-03e649213180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This copies the one-dimensional array 5 times on dimension 1\n",
        "extendedTensor = myTensor.repeat(5, 1)\n",
        "print(extendedTensor)\n",
        "print(extendedTensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.]])\n",
            "torch.Size([5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51P4ZRVLoxeo",
        "outputId": "3e7963bb-4222-4eab-df61-fc6569103b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "slicedTensor = extendedTensor[3, :]\n",
        "slicedTensor[:] = 5\n",
        "print(slicedTensor)\n",
        "print(slicedTensor.size())\n",
        "print(extendedTensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5., 5., 5., 5.])\n",
            "torch.Size([4])\n",
            "tensor([[1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [5., 5., 5., 5.],\n",
            "        [1., 2., 3., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyGZwW19pEJA"
      },
      "source": [
        "Notice how in the above example the slicedTensor still points to the original content of extendedTensor, modifying its values also modifies the contents in extendedTensor. If you wanted to rather create a copy of the values in the slice you can always use the .clone() method.\n",
        "\n",
        "You can review other tensor operations in pytorch's documentation here: http://pytorch.org/docs/master/tensors.html\n",
        "\n",
        "Other functions I often find useful besides repeat(), and clone(), are: view(), squeeze(), unsqueeze(), transpose(), and permute().\n",
        "\n",
        "**Excercise:** Run some examples to see what view(), squeeze(), unsqueeze(), transpose(), and permute() can do for you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Tensor Operations**\n",
        "Basic tensor operations include scalar, tensor multiplication, and addition. It also includes element-wise tensor-tensor operations, and other operations that might be specific to 2D tensors (matrices) such as matrix-matrix multiplication. Two things to notice 1) Basic operators have been overloaded so sometimes it is not needed to explicitly call a torch function 2) Many torch operations have an in-place version that operates in the same space of an input tensor as opposed to returning a new one, we show some examples below:"
      ],
      "metadata": {
        "id": "_ITMycMNDHak"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMtJcJX5o1xT",
        "outputId": "13704d0e-868c-4554-939b-b7ce3b006c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Adding a scalar to a tensor:\n",
        "inputTensor = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(inputTensor.add(1))\n",
        "print(inputTensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 3.],\n",
            "        [4., 5.]])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rbj0R4upYYS",
        "outputId": "729e1f59-3b74-4a5d-86d4-4e8cec84e866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Adding a scalar to a tensor (in-place):\n",
        "inputTensor = torch.Tensor([[1, 2], [3, 4]])\n",
        "# In-place operations usually are followed by underscore _\n",
        "print(inputTensor.add_(1))  \n",
        "print(inputTensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 3.],\n",
            "        [4., 5.]])\n",
            "tensor([[2., 3.],\n",
            "        [4., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTelScnCpnTT"
      },
      "source": [
        "The above code should show the difference between in-place operations, similarly there are mul(), and div() methods.\n",
        "\n",
        "The following code shows tensor-tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRsasEiXpe-o",
        "outputId": "367d881c-e13b-4038-cfad-91ba9aa88732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "myTensor1 = torch.zeros(3, 2, 2).fill_(2)\n",
        "myTensor2 = torch.zeros(3, 2, 2).fill_(3)\n",
        "print(myTensor1 + myTensor2)\n",
        "print(myTensor1 * myTensor2)\n",
        "# .addcmul(c, a, b) performs c + a .* b (Where .* means element-wise multiplication)\n",
        "print(torch.addcmul(myTensor1, myTensor1, myTensor2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[5., 5.],\n",
            "         [5., 5.]],\n",
            "\n",
            "        [[5., 5.],\n",
            "         [5., 5.]],\n",
            "\n",
            "        [[5., 5.],\n",
            "         [5., 5.]]])\n",
            "tensor([[[6., 6.],\n",
            "         [6., 6.]],\n",
            "\n",
            "        [[6., 6.],\n",
            "         [6., 6.]],\n",
            "\n",
            "        [[6., 6.],\n",
            "         [6., 6.]]])\n",
            "tensor([[[8., 8.],\n",
            "         [8., 8.]],\n",
            "\n",
            "        [[8., 8.],\n",
            "         [8., 8.]],\n",
            "\n",
            "        [[8., 8.],\n",
            "         [8., 8.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_7cqlfxqG9A"
      },
      "source": [
        "Other useful tensor-tensor operations are addcmul_() [in place version of addcmul()], matmul() (tensor-tensor multiplication), mm() (matrix multiplication), mv() (matrix-vector multiplication)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. GPU Computing**\n",
        "A powerful feature of Pytorch is that we can run operations on the GPU simply by first moving matrices or vectors (tensors) in and out of GPU memory. See example below."
      ],
      "metadata": {
        "id": "u1_6N2beDN-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "deviceGPU = torch.device('cuda')\n",
        "\n",
        "# Let's create two matrices of size dxd and full of random values.\n",
        "d = 5000\n",
        "a = torch.rand(d, d)\n",
        "b = torch.rand(d, d)\n",
        "\n",
        "# Compute a multiplication between the two matrices in CPU.\n",
        "start_time = time.time()\n",
        "output = torch.mm(a,b)  # Regular multiplication in CPU.\n",
        "cpu_time = time.time() - start_time\n",
        "print('CPU-time {0:.6f}s'.format(cpu_time))\n",
        "\n",
        "# Compute a multiplication between the two matrices in GPU.\n",
        "start_time = time.time()\n",
        "a = a.to(deviceGPU) # Move to GPU.\n",
        "b = b.to(deviceGPU) # Move to GPU.\n",
        "output = torch.mm(a,b)  # Multiplication happens in GPU.\n",
        "gpu_time = time.time() - start_time\n",
        "print('GPU-time {0:.6f}s\\n'.format(gpu_time))\n",
        "\n",
        "print('CPU was {0:.2f}x times the GPU time'.format(cpu_time / gpu_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_yCQgOgDQXk",
        "outputId": "fcd28a88-0c6b-47f9-f292-04afa44bb811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU-time 3.119798s\n",
            "GPU-time 0.045453s\n",
            "\n",
            "CPU was 68.64x times the GPU time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Please take note that when you run the code above for the first time, it might be the case that the GPU code runs surprisingly slower than the CPU code. This can happen because the first time you run this code you pay some penalty for \"warming up\" the GPU. However on subsequent runs you will see how significantly faster is the GPU computation time compared to CPU computation time e.g. 64x times faster."
      ],
      "metadata": {
        "id": "NKkSP1E7JEhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Gradient Computations**\n",
        "Another powerful feature of Pytorch is that it keeps track of the operations being performed on tensors e.g. a torch.tensor stores the operations that have been applied to it so that we can execute gradient computations in the reverse order of the operations following the chain rule of calculus. This is often called automatic differentation (autodiff). Here we demonstrate this feature."
      ],
      "metadata": {
        "id": "NDEnWVZ4DgGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance consider the following function.\n",
        "\n",
        "$$f(X, Y) = (x_1 - y_1)^2  + (x_2 - y_2)^2$$,\n",
        "\n",
        "where $X = [x_1, x_2]$ and $Y = [y_1, y_2]$. Now, let's say we're interested in computing $\\partial f / \\partial X = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}]$ and $\\partial f / \\partial Y = [\\frac{\\partial f}{\\partial y_1}, \\frac{\\partial f}{\\partial y_2}]$.\n",
        "\n",
        "If we solve for $\\frac{\\partial f}{\\partial x_1}$, we will find that:\n",
        "$$\\frac{\\partial f}{\\partial x_1}= 2x_1 -2y_1,$$\n",
        "\n",
        "\n",
        "So when $X = [1.0, 3.0]$ and $Y = [3.0, 2.0]$, we can see that ${\\partial f}/ {\\partial x_1} = -4$. \n",
        "\n",
        "In pytorch we can compute all gradients with respect to all variables involved in the computation as follows:"
      ],
      "metadata": {
        "id": "J5UPe9mPGt3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X, Y):\n",
        "  return torch.sum((X - Y) ** 2)\n",
        "\n",
        "X = torch.tensor([1.0, 3.0], requires_grad = True)\n",
        "Y = torch.tensor([3.0, 2.0], requires_grad = True)\n",
        "\n",
        "out = f(X, Y)\n",
        "\n",
        "out.backward() # Execute chain rule of calculus to find all partial derivatives.\n",
        "\n",
        "print(X.grad) # This should print df/dx_1, df/dx_2\n",
        "#print(Y.grad) # This should print df/dy_1, df/dy_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M06ksJOvDh5V",
        "outputId": "72f78156-f40f-4ccc-82df-1f604a7a7796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-4.,  2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Given the values of $X = [1.0, 3.0]$, $Y = [3.0, 2.0]$ and $Z=[5.0, 1.0]$. Compute using pytorch and print $\\partial f / \\partial z_2$ given that:\n",
        "\n",
        "$$f(X, V) = XX^\\intercal - 2XV^\\intercal + VV^\\intercal,$$\n",
        "$$ V = Y - X + U,$$\n",
        "$$ U = Z - (2X - Z)^2 + (Y - 3Z)^2$$\n",
        "\n",
        "Assume $X$, $Y$, and $Z$ are row-vectors."
      ],
      "metadata": {
        "id": "TyQE880MG4Lu"
      }
    }
  ]
}